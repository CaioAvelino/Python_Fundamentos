{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Python Fundamentos - Capítulo 12</font>\n",
    "\n",
    "## Download: http://github.com/dsacademybr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecção de Emoções em Imagens com Inteligência Artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/facial-keypoints-detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais Convolucionais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em redes neurais convolucionais, os dados de entrada são muitas vezes moldados como uma matriz 3D (número de canais, largura da imagem, altura), que preserva a relação espacial entre os pixels. Na figura abaixo, a imagem 3 é um único canal (tons de cinza) de dados, portanto, a dimensão de entrada é especificada como uma tupla (1, largura da imagem, altura da imagem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MNIST-flat](https://www.cntk.ai/jup/cntk103a_MNIST_input.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagens de cor de cena natural são frequentemente apresentadas como canais de cor Vermelho-Verde-Azul (RGB). A dimensão de entrada dessas imagens é especificada como uma tupla (3, largura da imagem, altura da imagem). Se houver dados de entrada RGB como uma varredura volumétrica com largura de volume, altura de volume e profundidade de volume representando os 3 eixos, o formato de dados de entrada será especificado por uma tupla de 4 valores (3, largura de volume, altura de volume, profundidade de volume). Desta forma, podemos especificar as imagens de entrada em espaço arbitrário de dimensão superior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![input-rgb](https://www.cntk.ai/jup/cntk103d_rgb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN é uma rede feedforward composta de diversas camadas de tal forma que a saída de uma camada torna-se a entrada para a próxima camada (semelhante ao MLP). Em MLP, todos os pares possíveis de pixels de entrada são conectados aos nós de saída com cada par tendo um peso, conduzindo assim a uma explosão combinatória de parâmetros a serem aprendidos e também aumentando a possibilidade de overfitting. As camadas de convolução aproveitam a disposição espacial dos pixels e aprendem vários filtros que reduzem significativamente a quantidade de parâmetros na rede. O tamanho do filtro é um parâmetro da camada de convolução.\n",
    "\n",
    "Nesta seção, apresentamos os fundamentos das operações de convolução. \n",
    "\n",
    "### Camada de Convolução\n",
    "\n",
    "Uma camada de convolução é um conjunto de filtros. Cada filtro é definido por uma matriz de peso (** W **) e bias ($ b $).\n",
    "\n",
    "![input-filter](https://www.cntk.ai/jup/cntk103d_filterset.png)\n",
    "\n",
    "Estes filtros são varridos através da imagem que realiza o dot product entre os pesos e o valor de entrada correspondente ($\\vec{x}^T$). O valor de bias é adicionado à saída do dot product e a soma resultante é opcionalmente mapeada através de uma função de ativação. Esse processo é ilustrado na seguinte animação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "Image(url=\"https://www.cntk.ai/jup/cntk103d_conv2d_final.gif\", width= 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As camadas de convolução incorporam as seguintes características-chave:\n",
    "\n",
    "  - Em vez de estar totalmente conectado a todos os pares de nós de entrada e saída, cada nó de convolução é ** conectado localmente ** a um subconjunto de nós de entrada localizados em uma região de entrada menor, também chamada de campo receptivo (RF). A figura acima ilustra pequenas regiões 3 x 3 na imagem como a região RF. No caso de uma imagem RGB, haveria três dessas 3 x 3 regiões, uma de cada um dos 3 canais de cor.\n",
    "   \n",
    "   \n",
    "   - Em vez de ter um único conjunto de pesos (como em uma camada Densa), camadas convolucionais têm vários conjuntos (mostrado na figura com várias cores), chamado ** filtros **. Cada filtro detecta características dentro de cada RF possível na imagem de entrada. A saída da convolução é um conjunto de sub-camadas `n` (mostradas na animação abaixo) onde ` n` é o número de filtros (consulte a figura acima).\n",
    "   \n",
    "     \n",
    "   - Dentro de uma subcamada, em vez de cada nó ter seu próprio conjunto de pesos, um único conjunto de ** pesos compartilhados ** são usados por todos os nós nessa subcamada. Isso reduz o número de parâmetros a serem aprendidos. Isso também abre a porta para vários aspectos da aprendizagem profunda que permitiu a construção de soluções muito práticas:\n",
    "     -- Manuseio de imagens maiores (digamos 512 x 512)\n",
    "     -- Tentando maiores tamanhos de filtro (correspondente a um RF maior) como 11 x 11\n",
    "     -- Aprender mais filtros (digamos 128)\n",
    "     -- Explorar arquiteturas mais profundas (mais de 100 camadas)\n",
    "     -- Alcançar a invariância de tradução (a capacidade de reconhecer um recurso independentemente de onde eles aparecem na imagem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strides e Padding\n",
    "\n",
    "** Como os filtros são posicionados? ** Em geral, os filtros são dispostos em telhas sobrepostas, da esquerda para a direita e de cima para baixo. Cada camada de convolução tem um parâmetro para especificar a `filter_shape`, especificando a largura e a altura do filtro no caso das imagens de cena mais naturais. Há um parâmetro (`strides`) que controla a distância até a etapa para a direita ao mover os filtros através de vários RF's em uma linha, e até que ponto para descer quando se move para a próxima linha. O parâmetro booleano `pad` controla se a entrada deve ser preenchida em torno das bordas para permitir um mosaico completo dos RFs perto das bordas.\n",
    "\n",
    "A animação acima mostra os resultados com um `filter_shape` = (3, 3),` strides` = (2, 2) e `pad` = False. As duas animações abaixo mostram os resultados quando `pad` é definido como True. Primeiro, com um passo de 2 e segundo tendo um passo de 1.\n",
    "\n",
    "Nota: a forma da saída é diferente entre as duas configurações. Muitas vezes a sua decisão de pad e os valores de stride é baseada na forma da camada de saída necessária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "# Plot images com strides de 2 e 1 e padding habilitado\n",
    "images = [(\"https://www.cntk.ai/jup/cntk103d_padding_strides.gif\" , 'Stride = 2'),\n",
    "          (\"https://www.cntk.ai/jup/cntk103d_same_padding_no_strides.gif\", 'Stride = 1')]\n",
    "\n",
    "for im in images:\n",
    "    print(im[1])\n",
    "    display(Image(url=im[0], width=200, height=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "\n",
    "Muitas vezes, é necessário controlar o número de parâmetros, especialmente em redes profundas. Para cada camada de saída da camada de convolução (cada camada, corresponde à saída de um filtro), pode-se ter uma camada de agrupamento (Pooling). As camadas de agrupamento são tipicamente introduzidas para:\n",
    "- Reduzir a dimensionalidade da camada anterior (acelerando a rede),\n",
    "- Torna o modelo mais tolerante a alterações no local do objeto na imagem. Por exemplo, mesmo quando um dígito é deslocado para um lado da imagem em vez de estar no meio.\n",
    "\n",
    "É comum inserir periodicamente uma camada de agrupamento entre as camadas Convolucionais sucessivas em uma arquitetura ConvNet. Sua função é reduzir progressivamente o tamanho espacial da representação para reduzir a quantidade de parâmetros e de computação na rede e, portanto, também para controlar o overfitting. A Camada de Agrupamento opera independentemente em cada fatia de profundidade da entrada e redimensiona-a espacialmente, usando a operação MAX. A forma mais comum é uma camada de pooling com filtros de tamanho 2x2 aplicado com um stride de 2 downsamples cada fatia de profundidade na entrada por 2 ao longo de largura e altura, descartando 75% das ativações. Cada operação MAX, neste caso, seria tomar um máximo de 4 números (pequena região 2x2 em alguma fatia de profundidade). A dimensão da profundidade permanece inalterada.\n",
    "\n",
    "Vale ressaltar que existem apenas duas variações comumente observadas na camada de Max Pooling encontradas na prática: Uma camada de agrupamento com F = 3, S = 2 (também chamada de pool de sobreposição) e mais comumente F = 2, S = 2. Agrupando tamanhos com campos receptivos maiores pode destruir a rede e travar a máquina.\n",
    "\n",
    "O cálculo em um nó de pooling é muito mais simples do que um nó de feedforward normal. Ele não tem peso, bias ou função de ativação. Ele usa uma função de agregação simples (como max ou average) para calcular sua saída. A função mais comumente usada é \"max\" - um nó de pooling máximo simplesmente fornece o máximo dos valores de entrada correspondentes à posição do filtro da entrada. A figura abaixo mostra os valores de entrada em uma região 4 x 4. A tamanho máximo da janela de agrupamento é 2 x 2 e começa a partir do canto superior esquerdo. O valor máximo dentro da janela torna-se a saída da região. Cada vez que o modelo é deslocado pela quantidade especificada pelo parâmetro stride (como mostrado na figura abaixo) e a operação de pooling máximo é repetida.\n",
    "![maxppool](https://cntk.ai/jup/201/MaxPooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede Convolucional Típica\n",
    "\n",
    "![mnist-conv-mp](http://www.cntk.ai/jup/conv103d_mnist-conv-mp.png)\n",
    "\n",
    "Uma CNN típica contém um conjunto de camadas alternadas de convolução e agrupamento (Pooling) seguido por uma camada de saída densa para a classificação. Você encontrará variantes desta estrutura em muitas redes profundas clássicas (VGG, AlexNet, etc.). Isto está em contraste com a rede MLP, que consiste em 2 camadas densas seguidas por uma camada de saída densa.\n",
    "\n",
    "As ilustrações são apresentadas no contexto de imagens bidimensionais (2D), mas o conceito e os componentes podem operar em qualquer dado dimensional. O esquema acima mostra 2 camadas de convolução e 2 camadas de agrupamento máximo. Uma estratégia típica é aumentar o número de filtros nas camadas mais profundas, reduzindo o tamanho espacial de cada camada intermediária. Camadas intermediárias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A figura a seguir ilustra o modelo que vamos construir. Observe que os parâmetros no modelo abaixo devem ser experimentados. Estes são frequentemente chamados de hiperparâmetros de rede. Aumentar a forma do filtro leva a um aumento no número de parâmetros do modelo, aumenta o tempo de computação e ajuda o modelo a se ajustar melhor aos dados. No entanto, corre-se o risco de [overfitting](https://en.wikipedia.org/wiki/Overfitting). Normalmente, o número de filtros nas camadas mais profundas é maior do que o número de filtros nas camadas anteriores. Escolhemos 8 e 16 como número de filtros para a primeira e segunda camadas, respectivamente. Estes hiperparâmetros devem ser experimentados durante a construção do modelo.\n",
    "\n",
    "![conv-only](https://www.cntk.ai/jup/cntk103d_convonly2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Compreendendo os parâmetros **:\n",
    "\n",
    "\n",
    "Nosso modelo tem duas camadas de convolução, cada uma com peso e bias. Isso adiciona até 4 tensores de parâmetro. Adicionalmente, a camada densa tem tensores de peso e de bias. Assim, os tensores de 6 parâmetros.\n",
    "\n",
    "Vamos agora contar o número de parâmetros:\n",
    "- * Primeira camada de convolução *: Existem 8 filtros cada um de tamanho (1 x 5 x 5) onde 1 é o número de canais na imagem de entrada. Isto adiciona até 200 valores na matriz de peso e 8 valores de bias.\n",
    "\n",
    "\n",
    "- * Segunda camada de convolução *: Existem 16 filtros cada um de tamanho (8 x 5 x 5) onde 8 é o número de canais na entrada para a segunda camada (= saída da primeira camada). Isto adiciona até 3200 valores na matriz de peso e 16 valores de bias.\n",
    "\n",
    "\n",
    "- * Última camada densa *: Existem 16 x 7 x 7 valores de entrada e produz 10 valores de saída correspondentes aos 10 dígitos no conjunto de dados MNIST. Isto corresponde a (16 x 7 x 7) x 10 valores de peso e 10 valores de bias.\n",
    "\n",
    "Adicionando estes acima dá os 11274 parâmetros no modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo e Treinando o Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo os Dados e Hyperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from modulos import utils\n",
    "from datetime import datetime\n",
    "from tensorflow.python.framework import ops\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão do TensorFlow\n",
    "# Para instalar a mesma versão do TF, use: \n",
    "# CPU: pip install tensorflow==1.8 (no prompt ou terminal)\n",
    "# GPU: pip install tensorflow_gpu==1.8 (no prompt ou terminal)\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "ops.reset_default_graph()\n",
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "tf.flags.DEFINE_string(\"data_dir\", \"dataset/\", \"Caminho para o diretório com dados de treino e de teste\")\n",
    "tf.flags.DEFINE_string(\"logs_dir\", \"modelo/\", \"Caminho para o diretório onde o modelo será gravado\")\n",
    "tf.flags.DEFINE_string(\"mode\", \"train\", \"mode: train (Default)/ test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparâmetros\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_ITERATIONS = 1000\n",
    "REGULARIZATION = 1e-3\n",
    "IMAGE_SIZE = 48\n",
    "NUM_LABELS = 7\n",
    "VALIDATION_PERCENT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções Auxiliares Para Construção do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_regularization_loss(W, b):\n",
    "    tf.add_to_collection(\"losses\", tf.nn.l2_loss(W))\n",
    "    tf.add_to_collection(\"losses\", tf.nn.l2_loss(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, stddev=0.02, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "    if name is None:\n",
    "        return tf.Variable(initial)\n",
    "    else:\n",
    "        return tf.get_variable(name, initializer=initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape, name=None):\n",
    "    initial = tf.constant(0.0, shape=shape)\n",
    "    if name is None:\n",
    "        return tf.Variable(initial)\n",
    "    else:\n",
    "        return tf.get_variable(name, initializer=initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construção do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotionCNN(dataset):\n",
    "    \n",
    "    # Camada de Convolução 1\n",
    "    with tf.name_scope(\"conv1\") as scope:\n",
    "        tf.summary.histogram(\"W_conv1\", weights['wc1'])\n",
    "        tf.summary.histogram(\"b_conv1\", biases['bc1'])\n",
    "        conv_1 = tf.nn.conv2d(dataset, weights['wc1'], strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        h_conv1 = tf.nn.bias_add(conv_1, biases['bc1'])\n",
    "        h_1 = tf.nn.relu(h_conv1)\n",
    "        h_pool1 = tf.nn.max_pool(h_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "        add_to_regularization_loss(weights['wc1'], biases['bc1'])\n",
    "\n",
    "    # Camada de Convolução 2\n",
    "    with tf.name_scope(\"conv2\") as scope:\n",
    "        tf.summary.histogram(\"W_conv2\", weights['wc2'])\n",
    "        tf.summary.histogram(\"b_conv2\", biases['bc2'])\n",
    "        conv_2 = tf.nn.conv2d(h_pool1, weights['wc2'], strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        h_conv2 = tf.nn.bias_add(conv_2, biases['bc2'])\n",
    "        h_2 = tf.nn.relu(h_conv2)\n",
    "        h_pool2 = tf.nn.max_pool(h_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "        add_to_regularization_loss(weights['wc2'], biases['bc2'])\n",
    "\n",
    "    # Camada Totalmente Conectada 1\n",
    "    with tf.name_scope(\"fc_1\") as scope:\n",
    "        prob = 0.5\n",
    "        image_size = IMAGE_SIZE // 4\n",
    "        h_flat = tf.reshape(h_pool2, [-1, image_size * image_size * 64])\n",
    "        tf.summary.histogram(\"W_fc1\", weights['wf1'])\n",
    "        tf.summary.histogram(\"b_fc1\", biases['bf1'])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_flat, weights['wf1']) + biases['bf1'])\n",
    "        h_fc1_dropout = tf.nn.dropout(h_fc1, prob)\n",
    "        \n",
    "    # Camada Totalmente Conectada 2\n",
    "    with tf.name_scope(\"fc_2\") as scope:\n",
    "        tf.summary.histogram(\"W_fc2\", weights['wf2'])\n",
    "        tf.summary.histogram(\"b_fc2\", biases['bf2'])\n",
    "        pred = tf.matmul(h_fc1_dropout, weights['wf2']) + biases['bf2']\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pesos e Bias do Modelo\n",
    "weights = {\n",
    "    'wc1': weight_variable([5, 5, 1, 32], name=\"W_conv1\"),\n",
    "    'wc2': weight_variable([3, 3, 32, 64],name=\"W_conv2\"),\n",
    "    'wf1': weight_variable([int((IMAGE_SIZE // 4) * (IMAGE_SIZE // 4)) * 64, 256],name=\"W_fc1\"),\n",
    "    'wf2': weight_variable([256, NUM_LABELS], name=\"W_fc2\")\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': bias_variable([32], name=\"b_conv1\"),\n",
    "    'bc2': bias_variable([64], name=\"b_conv2\"),\n",
    "    'bf1': bias_variable([256], name=\"b_fc1\"),\n",
    "    'bf2': bias_variable([NUM_LABELS], name=\"b_fc2\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, label):\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=label))\n",
    "    tf.summary.scalar('Entropy', cross_entropy_loss)\n",
    "    reg_losses = tf.add_n(tf.get_collection(\"losses\"))\n",
    "    tf.summary.scalar('Reg_loss', reg_losses)\n",
    "    return cross_entropy_loss + REGULARIZATION * reg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loss, step):\n",
    "    return tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(images, labels, step):\n",
    "    offset = (step * BATCH_SIZE) % (images.shape[0] - BATCH_SIZE)\n",
    "    batch_images = images[offset: offset + BATCH_SIZE]\n",
    "    batch_labels = labels[offset:offset + BATCH_SIZE]\n",
    "    return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para resultados de treinamento\n",
    "train_error_list = []\n",
    "train_step_list = []\n",
    "\n",
    "# Listas para resultados de validação\n",
    "valid_error_list = []\n",
    "valid_step_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv=None):\n",
    "    \n",
    "    # Carrega os dados\n",
    "    train_images, train_labels, valid_images, valid_labels, test_images = utils.read_data(FLAGS.data_dir)\n",
    "    \n",
    "    print(\"\\nTamanho do Dataset de Treino: %s\" % train_images.shape[0])\n",
    "    print('Tamanho do Dataset de Validação: %s' % valid_images.shape[0])\n",
    "    print(\"Tamanho do Dataset de Teste: %s\" % test_images.shape[0])\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    dropout_prob = tf.placeholder(tf.float32)\n",
    "    input_dataset = tf.placeholder(tf.float32, [None, IMAGE_SIZE, IMAGE_SIZE, 1], name=\"input\")\n",
    "    input_labels = tf.placeholder(tf.float32, [None, NUM_LABELS])\n",
    "\n",
    "    pred = emotionCNN(input_dataset)\n",
    "    output_pred = tf.nn.softmax(pred, name=\"output\")\n",
    "    loss_val = loss(pred, input_labels)\n",
    "    train_op = train(loss_val, global_step)\n",
    "\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        summary_writer = tf.summary.FileWriter(FLAGS.logs_dir, sess.graph)\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.get_checkpoint_state(FLAGS.logs_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print(\"Modelo Restaurado!\")\n",
    "\n",
    "        for step in range(MAX_ITERATIONS):\n",
    "            batch_image, batch_label = get_next_batch(train_images, train_labels, step)\n",
    "            feed_dict = {input_dataset: batch_image, input_labels: batch_label}\n",
    "\n",
    "            sess.run(train_op, feed_dict=feed_dict)\n",
    "            if step % 10 == 0:\n",
    "                train_loss, summary_str = sess.run([loss_val, summary_op], feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, global_step=step)\n",
    "                train_error_list.append(train_loss)\n",
    "                train_step_list.append(step)\n",
    "                print(\"Taxa de Erro no Treinamento: %f\" % train_loss)\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                valid_loss = sess.run(loss_val, feed_dict={input_dataset: valid_images, input_labels: valid_labels})\n",
    "                valid_error_list.append(valid_loss)\n",
    "                valid_step_list.append(step)\n",
    "                print(\"%s Taxa de Erro na Validação: %f\" % (datetime.now(), valid_loss))\n",
    "                saver.save(sess, FLAGS.logs_dir + 'model.ckpt', global_step=step)\n",
    "        \n",
    "        # Plot do erro durante o treinamento\n",
    "        plt.plot(train_step_list, train_error_list, 'r--', label='Erro no Treinamento Por Iteração', linewidth=4)\n",
    "        plt.title('Erro no Treinamento Por Iteração')\n",
    "        plt.xlabel('Iteração')\n",
    "        plt.ylabel('Erro no Treinamento')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot do erro durante a validação\n",
    "        plt.plot(valid_step_list, valid_error_list, 'r--', label='Erro na Validação Por Iteração', linewidth=4)\n",
    "        plt.title('Erro na Validação Por Iteração')\n",
    "        plt.xlabel('Iteração')\n",
    "        plt.ylabel('Erro na Validação')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()  \n",
    "\n",
    "print(train_error_list) \n",
    "print(valid_error_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()\n",
    "    print(\"Treinanento concluído\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para adquirir conhecimento técnico sólido e especializado em Deep Learning, Visão Computacional, Processamento de Linguagem Natural e outros temas relacionados à Inteligência Artificial, confira nosso programa completo: <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\">Formação Inteligência Artificial</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obrigado - Data Science Academy - <a href=\"http://facebook.com/dsacademybr\">facebook.com/dsacademybr</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
